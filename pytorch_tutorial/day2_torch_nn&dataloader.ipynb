{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "day2_torch_nn&dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jieunjeon/ml-implementation-pytorch/blob/main/pytorch_tutorial/day2_torch_nn%26dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21TlCc95rF-L"
      },
      "source": [
        "# 이웃집 토토치 파이토치 : Day 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIOz112trF-W"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    <p>📢 해당 게시물은 파이토치 공식 튜토리얼 중 <a href=\"https://tutorials.pytorch.kr/beginner/nn_tutorial.htmlhttps://tutorials.pytorch.kr/beginner/nn_tutorial.html\">TORCH.NN 이 실제로 무엇인가요?</a>와 <a href=\"https://tutorials.pytorch.kr/beginner/basics/intro.htmlhttps://tutorials.pytorch.kr/beginner/basics/intro.html\">파이토치 기본 익히기</a>를 재구성하여 작성되었습니다.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNUHTULsrF-Y"
      },
      "source": [
        "#### 주요 키워드\n",
        "- 신경망 설계 - nn.Module, sequence\n",
        "- train loop\n",
        "- optimization\n",
        "- loss\n",
        "- dataset, dataloader\n",
        "- cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDBVxQy5rF-Z"
      },
      "source": [
        "#### 목차\n",
        "1. Tensor를 사용한 신경망\n",
        "    1. 데이터 준비\n",
        "    2. 신경망 정의\n",
        "    3. training loop 정의\n",
        "2. torch.nn.functional 사용하기\n",
        "    1. torch.nn.functional 사용하기\n",
        "    2. nn.Module 을 이용하여 리팩토링 하기\n",
        "    3. nn.Linear 를 이용하여 리팩토링 하기\n",
        "3. optm을 이용하여 리팩토링 하기\n",
        "4. fit() 와 get_data() 생성하기\n",
        "5. CNN 으로 넘어가기\n",
        "    1. nn.Module\n",
        "    2. nn.Sequential\n",
        "    3. DataLoader 감싸기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfpFvs6krF-a"
      },
      "source": [
        "#### 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vshB8VkurF-b",
        "tags": []
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda\") # GPU에서 실행하려면 이 주석을 제거하세요"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6rfl8gyrF-e"
      },
      "source": [
        "## 1. Tensor를 사용한 신경망\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLUOEMGarF-f"
      },
      "source": [
        "Day1에서는 Tensor와 autograd를 이용하여 신경망을 구성하는 법을 익혔습니다. 해당 부분을 참고하여 아래의 조건을 만족하는 신경망을 구축하여 봅시다.\n",
        "\n",
        "- FashionMNIST 분류기(입력: 784, 출력: 10)\n",
        "- 가중치와 편향을 가지는 선형모델\n",
        "- 활성화 함수 : log softmax\n",
        "- 손실 함수 : negative log-likelihood)\n",
        "- 평가 지표 : accuracy\n",
        "- training loop에서는 아래의 동작을 수행\n",
        "    - 데이터에서 배치(64)를 선택 👉 DataLoader 사용\n",
        "    - 모델을 이용하여 예측 수행\n",
        "    - 손실 계산\n",
        "    - `backward()`를 이용하여 모델의 기울기 업데이트 👉 weight, bias 업데이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We3qysP6rF-h"
      },
      "source": [
        "### A. 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUl9orNkrF-i"
      },
      "source": [
        "Day1에서 사용되었던 [FashionMNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist) 데이터셋을 다운로드 받아 준비합니다. 다양한 실습을 위하여 이전에는 `TorchVison`에서 불러왔던 것과 달리, 데이터셋을 다운로드 받은 후 이를 직접 정의한 `CustomDataset`으로 읽어들이고자 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mhYAz3grF-j"
      },
      "source": [
        "#### a. 데이터 다운로드\n",
        "데이터를 현재 작업환경 아래의 `data`폴더에 다운로드 받습니다. 이때, `wget` 패키지를 사용하게 되는데 만약 현재 가상환경에 설치 되어 있지 않다면 아래의 pip 코드를 수행시켜 설치를 진행 합니다.\n",
        "\n",
        "FashionMNIST 데이터셋은 60000개의 학습용 데이터와 100000개의 테스트용 데이터로 구분되어 있습니다. 학습용 데이터 중 10%(6000개)를 validation set으로 분리하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQXS8x6FrF-j"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMAOPTcjrYVH"
      },
      "source": [
        "!mkdir ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCb2KH2urF-k",
        "tags": []
      },
      "source": [
        "import wget\n",
        "\n",
        "urls = ['http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
        "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
        "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
        "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz']\n",
        "\n",
        "for url in urls:\n",
        "    # data 폴더 하위에 url에 게재되어 있는 자료를 다운도르 받는다 \n",
        "    wget.download(url, './data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfMY4VD_rF-l",
        "tags": []
      },
      "source": [
        "# code source : https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
        "# gz 파일을 열어 이미지와 라벨을 읽어들여 반환해준다.\n",
        "def load_mnist(path, kind='train'):\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "        labels = labels.astype(np.int64)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images/255.0, labels\n",
        "\n",
        "\n",
        "x_train, y_train = load_mnist('data', kind='train')\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)\n",
        "x_test, y_test = load_mnist('data', kind='t10k')\n",
        "\n",
        "\n",
        "print(f'train : image{x_train.shape}/{type(x_train[0])}, label{y_train.shape}/{type(y_train[0])}')\n",
        "print(f'valid : image{x_val.shape}/{type(x_val[0])}, label{y_val.shape}/{type(y_val[0])}')\n",
        "print(f'test  : image{x_test.shape}/{type(x_test[0])}, label{y_test.shape}/{type(y_test[0])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAN7MTrF-q"
      },
      "source": [
        "학습 데이터셋 60000과 테스트 데이터셋 10000이 정상적으로 불러와졌습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_u9hP7RrF-q"
      },
      "source": [
        "#### b. 커스텀 데이터셋/데이터로더 정의\n",
        "커스텀 데이터셋에서는 `__init__`, `__len__`, `__getitem__`을 정의해야 하면 각 메서드에서는 다음과 같은 작업을 수행합니다.\n",
        "\n",
        "- `__init__` : 클래스의 멤버변수 x, y, transform, target_transform을 파라미터를 이용하여 초기화 합니다. 이때, x는 image 데이터/y는 라벨/transform은 이미지에 적용 될 transform/target_transform은 라벨에 적용 될 transform을 의미합니다.\n",
        "- `__len__` : 데이터셋의 샘플 개수를 반환합니다.\n",
        "- `__getitem__` : 주어진 인덱스에 해당하는 샘플을 (image, label) 쌍으로 반환합니다. 이때, image에는 transform을 적용하고 라벨에는 target_transform을 적용하여 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHgBu5KBrF-r",
        "tags": []
      },
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, x, y, transform=None, target_transform=None):\n",
        "        self.labels = [\"T-Shirt\",  \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
        "                       \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "        \n",
        "        self.x = x # 이미지\n",
        "        self.y = y # 라벨\n",
        "        self.transform = transform # 이미지에 적용 될 transform\n",
        "        self.target_transform = target_transform # 라벨에 적용 될 transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y) # 데이터셋의 샘플 개수\n",
        "\n",
        "    def __getitem__(self, idx): # 주어진 idx번째에 해당하는 샘플을 반환\n",
        "        img = self.x[idx]\n",
        "        label = self.y[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(img)\n",
        "        \n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqAWy6ojrF-s"
      },
      "source": [
        "training loop에서는 배치를 선택하는 구간이 존재합니다. 이를 수월하기 진행하기 위하여 위에서 `DataLoader`를 이용하려고 합니다. DataLoader가 어떤 기능을 하는지 가물가물 하시다면 아래의 설명을 확인해 보세요.\n",
        "\n",
        "<details>\n",
        "    <summary>DataLoader</summary>\n",
        "    <div markdown=\"1\">       \n",
        "      `DataLoader` 에 데이터셋을 불러온 뒤에는 필요에 따라 데이터셋을 순회(iterate)할 수 있습니다. 아래의 각 순회(iteration)는 (각각 `batch_size=64` 의 특징(feature)과 정답(label)을 포함하는) `train_features` 와 `train_labels` 의 묶음(batch)을 반환합니다. `shuffle=True` 로 지정했으므로, 모든 배치를 순회한 뒤 데이터가 섞입니다.\n",
        "(데이터 불러오기 순서를 보다 세밀하게(finer-grained) 제어하려면 <a href=\"https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler\" >Samplers</a>\n",
        "를 살펴보세요.)\n",
        "    </div>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOdVvzd5rF-s",
        "tags": []
      },
      "source": [
        "train_ds = CustomImageDataset(x_train, y_train, \n",
        "                              transform=torch.Tensor)\n",
        "val_ds = CustomImageDataset(x_val, y_val, \n",
        "                            transform=torch.Tensor)\n",
        "\n",
        "x_test = torch.Tensor(x_test).to(device)\n",
        "y_test = torch.from_numpy(y_test).to(device)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "train_dl, val_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTplQVgXrF-t"
      },
      "source": [
        "데이터 로더가 정상적으로 생성되었는지 배치 하나를 출력하여 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVYbwTRGrF-t",
        "tags": []
      },
      "source": [
        "def get_label_name(label): # 라벨(숫자)를 입력받아 해당 라벨의 의미를 반환\n",
        "    LABLE_NAMES = [\"T-Shirt\",  \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
        "                   \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "    return LABLE_NAMES[label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbswZ-hQrF-u",
        "tags": []
      },
      "source": [
        "def show_batch(features, labels):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    for i in range(1, 65):\n",
        "        img = features[i-1]\n",
        "        label = get_label_name(labels[i-1])\n",
        "        ax = fig.add_subplot(8, 8, i)\n",
        "        \n",
        "        \n",
        "        if device.type  == \"cuda\":\n",
        "            img = img.cpu()\n",
        "        ax.imshow(img.reshape(28,28), cmap=\"gray\")\n",
        "        ax.set_title(label)\n",
        "        ax.set_xticks([]), ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv38MmXzrF-u",
        "tags": []
      },
      "source": [
        "features, labels = next(iter(train_dl))\n",
        "features, labels = features.to(device), labels.to(device)\n",
        "show_batch(features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xYQCQXCrF-v"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <p><b>Q. `get_label_name(np.argmax(labels[i-1]))`는 인코딩 되어 있는 라벨값을 라벨의 이름으로 변환합니다. 여기서 argmax 함수가 하는 역할을 무엇인가요?</b></p>\n",
        "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Vtu4d7rF-v"
      },
      "source": [
        "### B. 신경망 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkG8xlFKrF-v"
      },
      "source": [
        "#### a. 가중치 정의\n",
        "\n",
        "PyTorch는 랜덤 또는 0으로만 이루어진 텐서를 생성하는 메서드를 제공하고, 우리는 간단한 선형 모델의 가중치(weights)와 절편(bias)을 생성하기 위해서 이것을 사용할 것입니다. 이들은 일반적인 텐서에 매우 특별한 한 가지가 추가된 것입니다.\n",
        "\n",
        "우리는 PyTorch에게 이들이 기울기(gradient)가 필요하다고 알려줍니다.\n",
        "이를 통해 PyTorch는 텐서에 행해지는 모든 연산을 기록하게 하고,\n",
        "따라서 *자동적으로* 역전파(back-propagation) 동안에 기울기를 계산할 수 있습니다!\n",
        "\n",
        "가중치에 대해서는 `requires_grad_` 를 초기화(initialization) **다음에** 설정합니다,\n",
        "왜냐하면 우리는 해당 단계가 기울기에 포함되는 것을 원치 않기 때문입니다.\n",
        "(PyTorch에서 `_` 다음에 오는 메서드 이름은 연산이 인플레이스(in-place)로 수행되는 것을 의미합니다.)\n",
        "\n",
        "<div class=\"alert alert-info\"><b>📌 Note</b><p><a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\"><u>Xavier initialisation</u></a> \n",
        "   기법을 이용하여 가중치를 초기화 합니다. Standard gaussian을 이용해 생성한 난수를 '입력의 수'로 나누어 스케일링 합니다. 이에 대해서는 cs231n lec6에서 다뤄지고 있으니 한번 살펴보시기 바랍니다.</p></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcg8KkzprF-v",
        "tags": []
      },
      "source": [
        "weights = torch.randn(784, 10, device=device) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, device=device, requires_grad=True)\n",
        "weights, bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqjbx7H1rF-w"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <p><b>Q. 위에서 정의한 가중치의 형상은 (784x10) 입니다. 왜 이렇게 정의되었을까요?</b></p>\n",
        "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AqREKsdrF-w"
      },
      "source": [
        "#### b. 모델 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJtuCVTJrF-w"
      },
      "source": [
        "PyTorch의 기울기를 자동으로 계산해주는 기능 덕분에, Python 표준 함수\n",
        "(또는 호출 가능한 객체)를 모델로 사용할 수 있습니다!\n",
        "그러므로 간단한 선형 모델을 만들기 위해서 단순한 행렬 곱셈과 브로드캐스트(broadcast) 덧셈을 사용하여 보겠습니다.\n",
        "\n",
        "또한, 우리는 활성화 함수(activation function)가 필요하므로,`log_softmax` 를 구현하고 사용할 것입니다. PyTorch에서 많은 사전 구현된 손실 함수(loss function), 활성화 함수들이 제공되지만, 일반적인 python을 사용하여 자신만의 함수를 쉽게 작성할 수 있음을 기억해주세요.\n",
        "\n",
        "PyTorch는 심지어 여러분의 함수를 위해서 빠른 GPU 또는 벡터화된 CPU 코드를 만들어줄 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkDVaK-krF-w",
        "tags": []
      },
      "source": [
        "def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcQSbzpFrF-w"
      },
      "source": [
        "<details>\n",
        "    <summary>log_softmax의 정의가 왜 저렇게 되지?-하는 의문에 생긴다면 아래의 링크들을 확인해 보세요.</summary>\n",
        "    <div markdown=\"1\">       \n",
        "        • <a href=\"https://forums.fast.ai/t/what-is-torch-nn-really/36206/6\">What is torch.nn really?</a><br>\n",
        "        • <a href=\"https://discuss.pytorch.org/t/log-softmax-function-in-pytorch-tutorial-example/52041\">“log_softmax function” in pytorch tutorial example</a>\n",
        "    </div>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K959uBDJrF-x"
      },
      "source": [
        "위에서, `@` 기호는 점곱(dot product) 연산을 나타냅니다.\n",
        "우리는 하나의 배치(batch) 데이터(이 경우에는 64개의 이미지들)에 대하여 함수를 호출할 것입니다.\n",
        "이것은 하나의 *포워드 전달(forward pass)* 입니다. 이 단계에서 우리는 무작위(random) 가중치로\n",
        "시작했기 때문에 우리의 예측이 무작위 예측보다 전혀 나은 점이 없을 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODSs5E02rF-x",
        "tags": []
      },
      "source": [
        "preds = model(features)  # 예측\n",
        "print(f'preds[0] : {preds[0]}\\npreds.shape : {preds.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNLIjx5QrF-x",
        "tags": []
      },
      "source": [
        "preds_label = torch.argmax(preds, -1)\n",
        "show_batch(features, preds_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgxgWAWFrF-y"
      },
      "source": [
        "#### c. loss\n",
        "\n",
        "여러분이 보시듯이, ``preds`` 텐서(tensor)는 텐서 값 외에도, 또한\n",
        "기울기 함수(gradient function)를 담고 있습니다.\n",
        "우리는 나중에 이것을 역전파(backpropagation)를 위해 사용할 것입니다.\n",
        "이제 손실함수(loss function)로 사용하기 위한 `음의 로그 우도(negative log-likelihood)`를\n",
        "구현합시다. (다시 말하지만, 우리는 표준 Python을 사용할 수 있습니다.)\n",
        "\n",
        "![](https://i.ibb.co/5WNgh0n/hU252jE.jpg)<br>\n",
        "[이미지 출처 : https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOrnYfhlrF-y",
        "tags": []
      },
      "source": [
        "def nll(pred, target):\n",
        "    return -pred[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = nll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FzMqJXSrF-y"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <p><b>Q. 위에서 정의한 nll의 동작 방식을 설명하여 주세요.</b></p>\n",
        "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjiC4AxbrF-y"
      },
      "source": [
        "우리의 무작위 모델에 대한 손실을 점검해봅시다, 그럼으로써 우리는 나중에 역전파 이후에 개선이 있는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MmvM5zUrF-z",
        "tags": []
      },
      "source": [
        "yb =labels\n",
        "print(loss_func(preds, yb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OZataeTrF-z"
      },
      "source": [
        "#### d. accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcAPNDIlrF-0"
      },
      "source": [
        "우리 모델의 정확도(accuracy)를 계산하기 위한 함수를 구현합시다. 매 예측마다, 만약 가장 큰 값의 인덱스가 목표값(target value)과 동일하다면, 그 예측은 올바른 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzndq0bCrF-0",
        "tags": []
      },
      "source": [
        "def accuracy(out, yb): # yb: y batch\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LscRM8grF-0"
      },
      "source": [
        "우리의 무작위 모델의 정확도를 점검해 봅시다, 그럼으로써 손실이 개선됨에 따라서 정확도가 개선되는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrFAVjV_rF-1",
        "tags": []
      },
      "source": [
        "print(accuracy(preds, yb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzDEX7zirF-1"
      },
      "source": [
        "### C. training loop(훈련 루프) 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TluM9RhPrF-2"
      },
      "source": [
        "이제 우리는 훈련 루프(training loop)를 실행할 수 있습니다. 매 반복마다, 우리는 다음을 수행할 것입니다:\n",
        "\n",
        "- 데이터의 미니배치를 선택\n",
        "- 모델을 이용하여 예측 수행\n",
        "- 손실 계산\n",
        "- ``loss.backward()`` 를 이용하여 모델의 기울기 업데이트, 이 경우에는, ``weights`` 와 ``bias``.\n",
        "\n",
        "이제 우리는 이 기울기들을 이용하여 가중치와 절편을 업데이트 합니다.\n",
        "우리는 이것을 ``torch.no_grad()`` 컨텍스트 매니져(context manager) 내에서 실행합니다,\n",
        "왜냐하면 이러한 실행이 다음 기울기의 계산에 기록되지 않기를 원하기 때문입니다.\n",
        "PyTorch의 자동 기울기(Autograd)가 어떻게 연산을 기록하는지\n",
        "[여기](https://pytorch.org/docs/stable/notes/autograd.html)에서 더 알아볼 수 있습니다.\n",
        "\n",
        "우리는 그러고나서 기울기를 0으로 설정합니다, 그럼으로써 다음 루프(loop)에 준비하게 됩니다.\n",
        "그렇지 않으면, 우리의 기울기들은 일어난 모든 연산의 누적 집계를 기록하게 되버립니다.\n",
        "(즉, ``loss.backward()`` 가 이미 저장된 것을 대체하기보단, 기존 값에 기울기를 *더하게* 됩니다).\n",
        "\n",
        "<div class=\"alert alert-info\"><b>📌 Tip</b><p>여러분들은 PyTorch 코드에 대하여 표준 python 디버거(debugger)를 사용할 수 있으므로, 매 단계마다 다양한 변수 값을 점검할 수 있습니다. 아래에서  `set_trace()` 를 주석 해제하여 사용해보세요.</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAZUjgePrF-4",
        "tags": []
      },
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "lr = 0.5  # 학습률(learning rate)\n",
        "epochs = 2  # 훈련에 사용할 에폭(epoch) 수\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_dl:\n",
        "#         set_trace()\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()\n",
        "            \n",
        "\n",
        "    # validation\n",
        "    with torch.no_grad():\n",
        "        val_loss, val_acc = 0, 0\n",
        "        for xb, yb in val_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            val_loss += loss_func(model(xb), yb)\n",
        "            val_acc += accuracy(model(xb), yb)\n",
        "            \n",
        "    print(f'epoch {epoch+1} >>> val loss({val_loss / len(val_dl):.7f}), acc({val_acc / len(val_dl):.7f})')\n",
        "            \n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iLTKeaNrF-2"
      },
      "source": [
        "지난 시간 기울기를 0으로 설정해 주어야하는 이유에 대하여 집고 넘어가지 않아 아래의 내용을 추가하였습니다. 코드는 sin 함수를 예측하는 신경망이고, 학습 과저에서 기울기를 0으로 설정하였을 때와 그렇지 않을 경우의 local gradient의 변화를 출력하고 있습니다. 코드와 실행결과를 보고 아래의 질문에 답해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E_9ks9HrF-5",
        "tags": []
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "def get_grad_hist(mode):\n",
        "    dtype = torch.float\n",
        "\n",
        "    x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "    y = torch.sin(x)\n",
        "\n",
        "    a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "    b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "    c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "    d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "    grad_log = [[], [], [], []]\n",
        "\n",
        "    learning_rate = 1e-6\n",
        "    for t in range(2000):\n",
        "        y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "        loss = (y_pred - y).pow(2).sum()\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            a -= learning_rate * a.grad\n",
        "            b -= learning_rate * b.grad\n",
        "            c -= learning_rate * c.grad\n",
        "            d -= learning_rate * d.grad\n",
        "\n",
        "            grad_log[0].append(float(a.grad))\n",
        "            grad_log[1].append(float(b.grad))\n",
        "            grad_log[2].append(float(c.grad))\n",
        "            grad_log[3].append(float(d.grad))\n",
        "\n",
        "            if mode == 1:\n",
        "                # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
        "                a.grad = None\n",
        "                b.grad = None\n",
        "                c.grad = None\n",
        "                d.grad = None\n",
        "                \n",
        "    return grad_log\n",
        "\n",
        "def show_grad(grad_log, title):\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "\n",
        "    label = ['a', 'b', 'c', 'd']\n",
        "    for i in range(1, 5):\n",
        "        ax = fig.add_subplot(2, 2, i)\n",
        "        ax.plot(range(2000), grad_log[i-1], label=label[i-1])\n",
        "        ax.set_title(label[i-1])\n",
        "   \n",
        "    fig.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "grad_log_with_reset_gradient = get_grad_hist(1)\n",
        "grad_log_with_acc_gradient = get_grad_hist(0)\n",
        "\n",
        "show_grad(grad_log_with_acc_gradient, 'gradient change(no reset)')\n",
        "show_grad(grad_log_with_reset_gradient, 'gradient change(reset)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1DTc5VvrF-6"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <p><b>Q. 위 코드의 실행결과에서 두 plot는 확연한 차이를 보입니다. 이런한 차이를 보이는 이유는 무엇이고 어떤 plot가 잘 학습되고 있는 모델의 plot일까요?</b>  <a href=\"https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\">tip</a></p>\n",
        "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bIlGrNrF-6"
      },
      "source": [
        "## 2. torch.nn을 이용한 리팩토링\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD5__qgibEQs"
      },
      "source": [
        "이제 우리는 코드를 리팩토링(refactoring) 하겠습니다, 그럼으로써 이전과 동일하지만,\n",
        "PyTorch의 ``nn`` 클래스의 장점을 활용하여 더 간결하고 유연하게 만들 것입니다.\n",
        "지금부터 매 단계에서, 우리는 코드를 더 짧고, 이해하기 쉽고, 유연하게 만들어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zejiCYUtbEQs"
      },
      "source": [
        "### A. torch.nn.functional 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR3NO_X4bEQs"
      },
      "source": [
        "처음이면서 우리의 코드를 짧게 만들기 가장 쉬운 단계는 직접 작성한 <u>활성화, 손실 함수</u>를 ``torch.nn.functional`` 의 함수로 대체하는 것입니다(관례에 따라, 일반적으로 ``F`` 네임스페이스(namespace)를 통해 임포트(import) 합니다)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PayrGwRbEQs"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVmrAXiLbEQs"
      },
      "source": [
        "이 모듈에는 ``torch.nn`` 라이브러리의 모든 함수가 포함되어 있습니다(라이브러리의 다른 부분에는 클래스가 포함되어 있습니다.) \n",
        "다양한 손실 및 활성화 함수 뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데 편리한 몇 가지 함수도 여기에서 찾을 수 있습니다.(컨볼루션(convolution) 연산, 선형(linear) 레이어, 등을 수행하는 함수도 있지만, 앞으로 보시겠지만 일반적으로 라이브러리의 다른 부분을 사용하여 더 잘 처리 할 수 있습니다.)\n",
        "\n",
        "만약 여러분들이 음의 로그 우도 손실(nll)과 로그 소프트맥스(log softmax) 활성화 함수를 사용하는 경우, Pytorch는 이 둘을 결합하는 단일 함수인 `F.cross_entropy`를 제공합니다. \n",
        "\n",
        "따라서 모델에서 활성화 함수를 제거할 수도 있습니다.\n",
        "- [[Pytorch] softmax와 log_softmax (그리고 CrossEntropyLoss)](https://junstar92.tistory.com/118)\n",
        "- [03. 소프트맥스 회귀의 비용 함수 구현하기](https://wikidocs.net/60572)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM_wayykrF-6",
        "tags": []
      },
      "source": [
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "    return xb @ weights + bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXsaqDhbrF-7"
      },
      "source": [
        "더이상 ``model`` 함수에서 ``log_softmax`` 를 호출하지 않고 있습니다.\n",
        "손실과 정확도과 이전과 동일한지 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NQSJgFdrF-7",
        "tags": []
      },
      "source": [
        "print(loss_func(model(x_test), y_test), accuracy(model(x_test), y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "q7kq6MLEbEQt"
      },
      "source": [
        "### B. nn.Module 을 이용하여 리팩토링 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Om_HBWjgbEQt"
      },
      "source": [
        "다음으로, 더 명확하고 간결한 훈련 루프를 위해 ``nn.Module`` 및 ``nn.Parameter`` 를 사용합니다.\n",
        "우리는 ``nn.Module`` (자체가 클래스이고 상태를 추척할 수 있는) 하위 클래스(subclass)를 만듭니다.\n",
        "이 경우에는, 포워드(forward) 단계에 대한 가중치, 절편, 그리고 메소드(method) 등을 유지하는\n",
        "클래스를 만들고자 합니다.\n",
        "``nn.Module`` 은 우리가 사용할 몇 가지 속성(attribute)과 메소드를 (``.parameters()`` 와\n",
        "``.zero_grad()`` 같은) 가지고 있습니다.\n",
        "\n",
        "<div class=\"alert alert-info\"><b>📌 Note</b><p>`nn.Module`은 PyTorch 의 특정 개념이고, 우리는 이 클래스를 많이 사용할 것입니다. `nn.Module`를 Python의 코드를 임포트하기 위한 코드 파일인 <a href=\"https://docs.python.org/3/tutorial/modules.html\">module</a>의 개념과 헷갈리지 말아주세요.</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Gtq-YEeMbEQt"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
        "        self.bias = nn.Parameter(torch.zeros(10))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return xb @ self.weights + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T23:02:40.235911Z",
          "iopub.status.busy": "2021-09-30T23:02:40.234914Z",
          "iopub.status.idle": "2021-09-30T23:02:40.258853Z",
          "shell.execute_reply": "2021-09-30T23:02:40.256891Z",
          "shell.execute_reply.started": "2021-09-30T23:02:40.235911Z"
        },
        "id": "zqcsX1rFbEQt"
      },
      "source": [
        "함수를 사용하는 대신에 이제는 오브젝트(object) 를 사용하기 때문에,\n",
        "먼저 모델을 인스턴스화(instantiate) 해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "F9ZubD_XbEQt"
      },
      "source": [
        "model = Mnist_Logistic()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naugN4tQbEQu"
      },
      "source": [
        "이제 우리는 이전과 동일한 방식으로 손실을 계산할 수 있습니다.\n",
        "여기서 ``nn.Module`` 오브젝트들은 마치 함수처럼 사용됩니다 (즉, 이들은 *호출가능* 합니다),\n",
        "그러나 배후에서 Pytorch 는 우리의 ``forward`` 메소드를 자동으로 호출합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "HQfomeBYbEQu"
      },
      "source": [
        "print(loss_func(model(x_test), y_test), accuracy(model(x_test), y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxZcGWujbEQu"
      },
      "source": [
        "이전에는 훈련 루프를 위해 이름 별로 각 매개변수(parameter)의 값을 업데이트하고 다음과 같이\n",
        "각 매개 변수에 대한 기울기들을 개별적으로 수동으로 0으로 제거해야 했습니다.\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    weights -= weights.grad * lr\n",
        "    bias -= bias.grad * lr\n",
        "    weights.grad.zero_()\n",
        "    bias.grad.zero_()\n",
        "```\n",
        "\n",
        "이제 우리는 model.parameters() 및 model.zero_grad() (모두 ``nn.Module`` 에 대해 PyTorch에 의해 정의됨)를 활용하여 이러한 단계를 더 간결하게 만들고, 특히 더 복잡한 모델에 대해서 일부 매개변수를 잊어 버리는 오류를 덜 발생시킬 수 있습니다.\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    for p in model.parameters(): p -= p.grad * lr\n",
        "        model.zero_grad()\n",
        "```\n",
        "\n",
        "이제 이것을 나중에 다시 실행할 수 있도록 ``fit`` 함수로 작은 훈련 루프를 감쌀 것입니다.\n",
        "\n",
        "훈련 전에 항상 `model.train()`을 호출하고, 추론(inference) 전에 `model.eval()`을 호출합니다, 이는 `nn.BatchNorm2d` 및 `nn.Dropout`과 같은 레이어에서 이러한 다른 단계(훈련, 추론) 에 대한 적절한 동작이 일어나게 하기 위함입니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "zt1iXErnbEQu"
      },
      "source": [
        "def fit(model, train_dl, val_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_acc = 0, 0\n",
        "            for xb, yb in val_dl:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                val_loss += loss_func(model(xb), yb)\n",
        "                val_acc += accuracy(model(xb), yb)\n",
        "\n",
        "            print(f'epoch {epoch+1} >>> val loss({val_loss / len(val_dl):.7f}), acc({val_acc / len(val_dl):.7f})')\n",
        "\n",
        "\n",
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "lr = 0.5  # 학습률(learning rate)\n",
        "epochs = 2  # 훈련에 사용할 에폭(epoch) 수\n",
        "fit(model, train_dl, val_dl)\n",
        "\n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T23:11:38.015066Z",
          "iopub.status.busy": "2021-09-30T23:11:38.015066Z",
          "iopub.status.idle": "2021-09-30T23:11:38.028031Z",
          "shell.execute_reply": "2021-09-30T23:11:38.026035Z",
          "shell.execute_reply.started": "2021-09-30T23:11:38.015066Z"
        },
        "id": "_Oqs8dX0bEQu"
      },
      "source": [
        "### C. nn.Linear 를 이용하여 리팩토링 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_vFGn0MbEQu"
      },
      "source": [
        "계속해서 코드를 리팩토링 합니다. `self.weights` 및 `self.bias`를 수동으로 정의 및 초기화하고, `xb  @ self.weights + self.bias`를 계산하는 대신에, 위의 모든 것을 해줄 Pytorch 클래스인 [nn.Linear](https://pytorch.org/docs/stable/nn.html#linear-layers)를 선형 레이어로 사용합니다.\n",
        "\n",
        "Pytorch에는 다양한 유형의 코드를 크게 단순화 할 수 있는 미리 정의된 레이어가 있고 이는 또한 종종 기존 코드보다 속도를 빠르게 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eain_A2cbEQv"
      },
      "source": [
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLl6VifbEQv"
      },
      "source": [
        "이전과 같은 방식으로 모델을 인스턴스화하고 손실을 계산합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "_yEXllmzbEQv"
      },
      "source": [
        "model = Mnist_Logistic().to(device)\n",
        "print(loss_func(model(x_test), y_test), accuracy(model(x_test), y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBIiaBMNbEQv"
      },
      "source": [
        "우리는 여전히 이전과 동일한 `fit` 메소드를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "V0Qb8XOpbEQv"
      },
      "source": [
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "lr = 0.5  # 학습률(learning rate)\n",
        "epochs = 2  # 훈련에 사용할 에폭(epoch) 수\n",
        "fit(model, train_dl, val_dl)\n",
        "\n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSVzrGQ6bEQv"
      },
      "source": [
        "## 3. optm을 이용하여 리팩토링 하기\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfyizBnfbEQw"
      },
      "source": [
        "Pytorch에는 다양한 최적화(optimization) 알고리즘을 가진 패키지인 `torch.optim`도 있습니다.\n",
        "각 매개변수를 수동으로 업데이트 하는 대신, 옵티마이저(optimizer)의 `step` 메소드를 사용하여\n",
        "업데이트를 진행할 수 있습니다.\n",
        "\n",
        "이렇게 하면 이전에 수동으로 코딩한 최적화 단계를 대체할 수 있습니다:\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    for p in model.parameters(): p -= p.grad * lr\n",
        "    model.zero_grad()\n",
        "```\n",
        "\n",
        "대신에 이렇게 말이죠:\n",
        "```python\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "```\n",
        "\n",
        "(`optim.zero_grad()` 는 기울기를 0으로 재설정 해줍니다. 다음 미니 배치에 대한 기울기를 계산하기 전에 호출해야 합니다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QyQlGGmbEQw"
      },
      "source": [
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPYS8HuQbEQw"
      },
      "source": [
        "나중에 다시 사용할 수 있도록 모델과 옵티마이져를 만드는 작은 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "iCUm1c2SbEQw"
      },
      "source": [
        "def get_model():\n",
        "    model = Mnist_Logistic().to(device)\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ixK2iePLbEQw"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss, val_acc = 0, 0\n",
        "        for xb, yb in val_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            val_loss += loss_func(model(xb), yb)\n",
        "            val_acc += accuracy(model(xb), yb)\n",
        "\n",
        "    print(f'epoch {epoch+1} >>> val loss({val_loss / len(val_dl):.7f}), acc({val_acc / len(val_dl):.7f})')\n",
        "        \n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i33fIHO8bEQw"
      },
      "source": [
        "## 4. fit() 와 get_data() 생성하기\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3CHklr-bEQx"
      },
      "source": [
        "이제 우리는 우리만의 작은 리팩토링을 수행할 것입니다.\n",
        "훈련 데이터셋과 검증 데이터셋 모두에 대한 손실을 계산하는 유사한 프로세스를 두 번 거치므로,\n",
        "이를 하나의 배치에 대한 손실을 계산하는 자체 함수 ``loss_batch`` 로 만들어보겠습니다.\n",
        "\n",
        "훈련 데이터셋에 대한 옵티마이저를 전달하고 이를 사용하여 역전파를 수행합니다.\n",
        "검증 데이터셋의 경우 옵티마이저를 전달하지 않으므로 메소드가 역전파를 수행하지 않습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lHy0X2RbEQx"
      },
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CDTnRYzbEQx"
      },
      "source": [
        "`fit` 은 모델을 훈련하고 각 에폭에 대한 훈련 및 검증 손실을 계산하는 작업을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "wR7FnylibEQx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, val_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in val_dl]\n",
        "            )\n",
        "\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "        print(epoch, val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "k3LnlbpZbEQx"
      },
      "source": [
        "def get_data(train_ds, valid_ds, bs):\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(valid_ds, batch_size=bs*2), # 검증 데이터셋에 대해서는 역전파(backpropagation)가 필요하지 않으므로 \n",
        "                                               # 메모리를 덜 사용하기 때문에 배치 사이즈를 늘릴 수 있다.\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T23:39:14.705123Z",
          "iopub.status.busy": "2021-09-30T23:39:14.704128Z",
          "iopub.status.idle": "2021-09-30T23:39:14.716056Z",
          "shell.execute_reply": "2021-09-30T23:39:14.714096Z",
          "shell.execute_reply.started": "2021-09-30T23:39:14.705123Z"
        },
        "id": "-ZEKkuDfbEQx"
      },
      "source": [
        "`get_data`는 학습 및 검증 데이터셋에 대한 dataloader 를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "_gWjjG9JbEQx"
      },
      "source": [
        "bs = 64\n",
        "train_dl, val_dl = get_data(train_ds, val_ds, bs)\n",
        "model, opt = get_model()\n",
        "\n",
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, val_dl)\n",
        "\n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejh_NKAtbEQx"
      },
      "source": [
        "이러한 기본 3줄의 코드를 사용하여 다양한 모델을 훈련할 수 있습니다.<br>\n",
        "컨볼루션 신경망(CNN)을 훈련하는 데 사용할 수 있는지 살펴 보겠습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fludhfbRbEQx"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h2>📌 중간정리</h2>\n",
        "    <p>지금까지 MNIST Fashion dataset을 분류하는 간단한 선형 분류기를 아래의 단계를 걸치며 변형시켜 보았습니다.</p>\n",
        "    <div>\n",
        "        • Tensor와 autograd만을 이용한 신경망 구축<br>\n",
        "        • nn.functional을 이용하여 활성화 함수/손실 함수 대체 👉 해당 함수들을 직접 작성하지 않아도 되게 됨<br>\n",
        "        • nn.Module를 이용하여 모델을 모듈화 👉 gradient 초기화가 간편해짐<br>\n",
        "        • nn.Linear를 이용하여 모델 정의 👉 가중치/편향 초기화와 forward를 직접 작성하지 않아도 되게 됨<br>\n",
        "        • optm을 이용하여 가중치 업데이트\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T23:53:58.762438Z",
          "iopub.status.busy": "2021-09-30T23:53:58.761438Z",
          "iopub.status.idle": "2021-09-30T23:53:58.776395Z",
          "shell.execute_reply": "2021-09-30T23:53:58.774434Z",
          "shell.execute_reply.started": "2021-09-30T23:53:58.762438Z"
        },
        "id": "SMdAKe8qbEQy"
      },
      "source": [
        "## 5. CNN 으로 넘어가기\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kcLzFnjbEQy"
      },
      "source": [
        "이제 3개의 컨볼루션 레이어로 신경망을 구축할 것입니다.\n",
        "이전 섹션의 어떤 함수도 모델의 형식에 대해 가정하지 않기 때문에,\n",
        "별도의 수정없이 CNN을 학습하는 데 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJxIP3RtbEQy"
      },
      "source": [
        "### A. nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY7IRB1GbEQy"
      },
      "source": [
        "Pytorch 의 사전정의된 [Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) 클래스를 컨볼루션 레이어로 사용합니다. 3개의 컨볼루션 레이어로 CNN을 정의합니다.\n",
        "각 컨볼루션 뒤에는 ReLU가 있습니다. 마지막으로 평균 풀링(average pooling)을 수행합니다.(`view` 는 PyTorch의 numpy `reshape` 버전입니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykAPgE4DbEQy"
      },
      "source": [
        "class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.view(-1, 1, 28, 28)\n",
        "        xb = F.relu(self.conv1(xb))\n",
        "        xb = F.relu(self.conv2(xb))\n",
        "        xb = F.relu(self.conv3(xb))\n",
        "        xb = F.avg_pool2d(xb, 4)\n",
        "        return xb.view(-1, xb.size(1))\n",
        "\n",
        "lr = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T23:55:57.953135Z",
          "iopub.status.busy": "2021-09-30T23:55:57.952181Z",
          "iopub.status.idle": "2021-09-30T23:55:57.964104Z",
          "shell.execute_reply": "2021-09-30T23:55:57.962110Z",
          "shell.execute_reply.started": "2021-09-30T23:55:57.952181Z"
        },
        "id": "IHelDglRbEQy"
      },
      "source": [
        "[모멘텀(Momentum)](https://cs231n.github.io/neural-networks-3/#sgd)은 이전 업데이트도 고려하고 일반적으로 더 빠른 훈련으로 이어지는 확률적 경사하강법(stochastic gradient descent)\n",
        "의 변형입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "BTTB-udMbEQy"
      },
      "source": [
        "model = Mnist_CNN()\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, val_dl)\n",
        "\n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Ki56XsbEQy"
      },
      "source": [
        "### B. nn.Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpKj_DBabEQz"
      },
      "source": [
        "`torch.nn`에는 코드를 간단히 사용할 수 있는 또 다른 편리한 클래스인[Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)이 있습니다.\n",
        "`Sequential` 객체는 그 안에 포함된 각 모듈을 순차적으로 실행합니다. 이것은 우리의 신경망을 작성하는 더 간단한 방법입니다.\n",
        "\n",
        "\n",
        "이를 활용하려면 주어진 함수에서 **사용자정의 레이어(custom layer)** 를 쉽게 정의할 수 있어야 합니다. 예를 들어, PyTorch에는 `view` 레이어가 없으므로 우리의 신경망 용으로 만들어야 합니다.\n",
        "`Lambda` 는 `Sequential` 로 신경망을 정의할 때 사용할 수 있는 레이어를 생성할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSpNM92bEQz"
      },
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "    return x.view(-1, 1, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T23:58:34.931240Z",
          "iopub.status.busy": "2021-09-30T23:58:34.930243Z",
          "iopub.status.idle": "2021-09-30T23:58:34.950191Z",
          "shell.execute_reply": "2021-09-30T23:58:34.948193Z",
          "shell.execute_reply.started": "2021-09-30T23:58:34.931240Z"
        },
        "id": "d_0ADJcgbEQ0"
      },
      "source": [
        "`Sequential` 로 생성된 모델은 간단하게 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5kc_GPXTbEQ0"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    Lambda(preprocess),\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(4),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "print(f'[before] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[before] acc : {accuracy(model(x_test), y_test)}')\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, val_dl)\n",
        "\n",
        "print(f'[after ] loss : {loss_func(model(x_test), y_test)}')\n",
        "print(f'[after ] acc : {accuracy(model(x_test), y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWqYKNzibEQ0"
      },
      "source": [
        "### C. DataLoader 감싸기\n",
        "---\n",
        "\n",
        "우리의 CNN은 상당히 간결하지만, MNIST에서만 작동합니다, 왜냐하면:\n",
        " - 입력이 28\\*28의 긴 벡터라고 가정합니다.\n",
        " - 최종적으로 CNN 그리드 크기는 4\\*4 라고 가정합니다. (이것은 우리가 사용한 평균 풀링 커널 크기 때문입니다.)\n",
        "\n",
        "이 두 가지 가정을 제거하여 모델이 모든 2d 단일 채널(channel) 이미지에서 작동하도록 하겠습니다.\n",
        "먼저 초기 Lambda 레이어를 제거하고 데이터 전처리를 제네레이터(generator)로 이동시킬 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ou_9lx-5bEQ0"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28), y\n",
        "\n",
        "\n",
        "class WrappedDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "train_dl, val_dl = get_data(train_ds, val_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "val_dl = WrappedDataLoader(val_dl, preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AzbL7b0bEQ0"
      },
      "source": [
        "다음으로 ``nn.AvgPool2d`` 를 ``nn.AdaptiveAvgPool2d`` 로 대체하여 우리가 가진\n",
        "*입력* 텐서가 아니라 원하는 *출력* 텐서의 크기를 정의할 수 있습니다.\n",
        "결과적으로 우리 모델은 모든 크기의 입력과 함께 작동합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "XCNpms3IbEQ1"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T5b65ejbEQ1"
      },
      "source": [
        "한번 실행해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "x7e8OGnPbEQ1"
      },
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, val_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkgq7mKfbEQ1"
      },
      "source": [
        "## 마치면서\n",
        "---\n",
        "\n",
        "이제 Pytorch를 사용하여 다양한 유형의 모델을 학습하는 데 사용할 수 있는 일반 데이터 파이프 라인과\n",
        "훈련 루프가 있습니다.\n",
        "이제 모델 학습이 얼마나 간단한지 확인하려면 `mnist_sample` 샘플 노트북을 살펴보세요.\n",
        "\n",
        "물론 데이터 증강(data augmentation), 초매개변수 조정(hyperparameter tuning),\n",
        "훈련과정 모니터링(monitoring training), 전이 학습(transfer learning) 등과 같이\n",
        "추가하고 싶은 항목들이 많이 있을 것입니다.\n",
        "이러한 기능들은 이 튜토리얼에 표시된 것과 동일한 설계 접근 방식을 사용하여 개발된 fastai 라이브러리에서\n",
        "사용할 수 있으며, 모델을 더욱 발전시키려는 실무자에게 자연스러운 다음 단계를 제공합니다.\n",
        "\n",
        "이 튜토리얼의 시작 부분에서 ``torch.nn``, ``torch.optim``, ``Dataset``,\n",
        "그리고 ``DataLoader`` 의 각 예제를 통해 설명하겠다고 이야기했었습니다.\n",
        "이제 위의 내용들을 요약해보겠습니다:\n",
        "\n",
        " - **torch.nn**\n",
        "\n",
        "   + ``Module``: 함수처럼 동작하지만, 또한 상태(state) (예를 들어, 신경망의 레이어 가중치)를\n",
        "     포함할 수 있는 호출 가능한 오브젝트를 생성합니다.\n",
        "     이는 포함된 ``Parameter`` (들)가 어떤 것인지 알고, 모든 기울기를 0으로 설정하고 가중치\n",
        "     업데이트 등을 위해 반복할 수 있습니다.\n",
        "   + ``Parameter``: ``Module`` 에 역전파 동안 업데이트가 필요한 가중치가 있음을 알려주는\n",
        "     텐서용 래퍼입니다. `requires_grad` 속성이 설정된 텐서만 업데이트 됩니다.\n",
        "   + ``functional``: 활성화 함수, 손실 함수 등을 포함하는 모듈 (관례에 따라 일반적으로\n",
        "     ``F`` 네임스페이스로 임포트 됩니다) 이고, 물론 컨볼루션 및 선형 레이어 등에 대해서\n",
        "     상태를 저장하지않는(non-stateful) 버전의 레이어를 포함합니다.\n",
        " - ``torch.optim``: 역전파 단계에서 ``Parameter`` 의 가중치를 업데이트하는,\n",
        "   ``SGD`` 와 같은 옵티마이저를 포함합니다.\n",
        " - ``Dataset``: ``TensorDataset`` 과 같이 Pytorch와 함께 제공되는 클래스를 포함하여 ``__len__`` 및\n",
        "   ``__getitem__`` 이 있는 객체의 추상 인터페이스\n",
        " - ``DataLoader``: 모든 종류의 ``Dataset`` 을 기반으로 데이터의 배치들을 출력하는 반복자(iterator)를 생성합니다.\n",
        "\n"
      ]
    }
  ]
}